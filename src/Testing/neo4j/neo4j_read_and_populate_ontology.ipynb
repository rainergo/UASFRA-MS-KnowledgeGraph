{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Ontology Driven KG creation",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyPa90V5qw4Hy/CLydkkxIwD",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import rdflib\n",
    "import time\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from settings import path_base"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "path_to_secrets = pathlib.Path(path_base, 'secrets.env')\n",
    "try:\n",
    "    load_dotenv(dotenv_path=path_to_secrets)  # Load secrets/env variables\n",
    "except:\n",
    "    print('secrets could not be loaded!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#utility function to get the local part of a URI (stripping out the namespace)\n",
    "\n",
    "def getLocalPart(uri):\n",
    "  pos = -1\n",
    "  pos = uri.rfind('#') \n",
    "  if pos < 0 :\n",
    "    pos = uri.rfind('/')  \n",
    "  if pos < 0 :\n",
    "    pos = uri.rindex(':')\n",
    "  return uri[pos+1:]\n",
    "\n",
    "def getNamespacePart(uri):\n",
    "  pos = -1\n",
    "  pos = uri.rfind('#') \n",
    "  if pos < 0 :\n",
    "    pos = uri.rfind('/')  \n",
    "  if pos < 0 :\n",
    "    pos = uri.rindex(':')\n",
    "  return uri[0:pos+1]\n",
    "\n",
    "# quick test\n",
    "print(getLocalPart(\"http://onto.neo4j.com/rail#Station\"))\n",
    "print(getNamespacePart(\"http://onto.neo4j.com/rail#Station\"))"
   ],
   "metadata": {
    "id": "eRYK8Uvk67jQ"
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station\n",
      "http://onto.neo4j.com/rail#\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Own ontology:\n",
    "# path_to_ontology = pathlib.Path(path_base, \"models/Ontologies/Ontology3.ttl\").as_posix()\n",
    "path_to_ontology = pathlib.Path(path_base, \"models/Ontologies/rail.ttl\").as_posix()\n",
    "# From tutorial:\n",
    "# path_to_ontology = \"https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/ontos/rail.ttl\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI:  http://onto.neo4j.com/rail#Event \n",
      "CLASS:  Event \n",
      "NAMESPACE:  http://onto.neo4j.com/rail#\n",
      "-----------------------------------------------------------\n",
      "URI:  http://onto.neo4j.com/rail#Station \n",
      "CLASS:  Station \n",
      "NAMESPACE:  http://onto.neo4j.com/rail#\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "g = rdflib.Graph()\n",
    "g.parse(source=path_to_ontology, format='turtle')\n",
    "\n",
    "simple_query = \"\"\"\n",
    "prefix owl: <http://www.w3.org/2002/07/owl#>\n",
    "prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> \n",
    "\n",
    "SELECT DISTINCT ?c\n",
    "  WHERE {\n",
    "    ?c rdf:type owl:Class .\n",
    "  } \"\"\"\n",
    "\n",
    "for row in g.query(simple_query):\n",
    "    print('URI: ', str(row.c),'\\nCLASS: ',getLocalPart(str(row.c)),'\\nNAMESPACE: ',getNamespacePart(str(row.c)))\n",
    "    # print(str(row.c))\n",
    "    # print(str(getLocalPart(str(row.c))))\n",
    "    print('-----------------------------------------------------------')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rdflib.term.Variable('curi'), rdflib.term.Variable('props')]\n"
     ]
    }
   ],
   "source": [
    "# read the onto and generate cypher (complete without mappings)\n",
    "\n",
    "g = rdflib.Graph()\n",
    "g.parse(path_to_ontology, format='turtle')\n",
    "\n",
    "classes_and_props_query = \"\"\"\n",
    "prefix owl: <http://www.w3.org/2002/07/owl#> \n",
    "prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "SELECT DISTINCT ?curi (GROUP_CONCAT(DISTINCT ?propTypePair ; SEPARATOR=\",\") AS ?props)\n",
    "WHERE {\n",
    "    ?curi rdf:type owl:Class .\n",
    "    optional { \n",
    "      ?prop rdfs:domain ?curi ;\n",
    "        a owl:DatatypeProperty ;\n",
    "        rdfs:range ?range .\n",
    "      BIND (concat(str(?prop),';',str(?range)) AS ?propTypePair)\n",
    "    }\n",
    "  } GROUP BY ?curi  \"\"\"\n",
    "\n",
    "query_result_classes_and_props = g.query(classes_and_props_query)\n",
    "print(query_result_classes_and_props.vars)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{rdflib.term.Variable('curi'): rdflib.term.URIRef('http://onto.neo4j.com/rail#Event'), rdflib.term.Variable('props'): rdflib.term.Literal('http://onto.neo4j.com/rail#eventDescription;http://www.w3.org/2001/XMLSchema#string,http://onto.neo4j.com/rail#eventId;http://www.w3.org/2001/XMLSchema#string,http://onto.neo4j.com/rail#eventType;http://www.w3.org/2001/XMLSchema#string')}\n",
      "-------------\n",
      "{rdflib.term.Variable('curi'): rdflib.term.URIRef('http://onto.neo4j.com/rail#Station'), rdflib.term.Variable('props'): rdflib.term.Literal('http://onto.neo4j.com/rail#lat;http://www.w3.org/2001/XMLSchema#float,http://onto.neo4j.com/rail#long;http://www.w3.org/2001/XMLSchema#float,http://onto.neo4j.com/rail#stationAddress;http://www.w3.org/2001/XMLSchema#string,http://onto.neo4j.com/rail#stationCode;http://www.w3.org/2001/XMLSchema#string,http://onto.neo4j.com/rail#stationName;http://www.w3.org/2001/XMLSchema#string')}\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for binding in query_result_classes_and_props.bindings:\n",
    "    print(binding)\n",
    "    print('-------------')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unwind $records AS record \n",
      "merge (n:Event { `<id_prop>`: record.`<col with id>`} ) \n",
      "set n.eventDescription = record.`<col with value for eventDescription>` \n",
      "set n.eventId = record.`<col with value for eventId>` \n",
      "set n.eventType = record.`<col with value for eventType>` \n",
      "return count(*) as total\n",
      "-----------\n",
      "unwind $records AS record \n",
      "merge (n:Station { `<id_prop>`: record.`<col with id>`} ) \n",
      "set n.lat = record.`<col with value for lat>` \n",
      "set n.long = record.`<col with value for long>` \n",
      "set n.stationAddress = record.`<col with value for stationAddress>` \n",
      "set n.stationCode = record.`<col with value for stationCode>` \n",
      "set n.stationName = record.`<col with value for stationName>` \n",
      "return count(*) as total\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "cypher_list = []\n",
    "\n",
    "for row in query_result_classes_and_props:\n",
    "    cypher = []\n",
    "    cypher.append(\"unwind $records AS record\")\n",
    "    cypher.append(\"merge (n:\" + getLocalPart(row.curi) + \" { `<id_prop>`: record.`<col with id>`} )\")\n",
    "    # print('----- OUTER -----')\n",
    "    # print('row.curi:', row.curi)\n",
    "    # print('     ----- inner ------')\n",
    "    # print('row.props:', row.props)\n",
    "    for pair in row.props.split(\",\"):\n",
    "        propName = pair.split(\";\")[0]\n",
    "        # print('propName:', propName)\n",
    "        propType = pair.split(\";\")[1]\n",
    "        # print('propType:', propType)\n",
    "        cypher.append(\"set n.\" + getLocalPart(propName) + \" = record.`<col with value for \" + getLocalPart(propName) + \">`\")\n",
    "        # print('getLocalPart(propName):', getLocalPart(propName))\n",
    "        # print('     ----- inner ------')\n",
    "    # print('----- OUTER -----')\n",
    "    cypher.append(\"return count(*) as total\") \n",
    "    cypher_list.append(' \\n'.join(cypher))\n",
    "\n",
    "for cypher in cypher_list:\n",
    "    print(cypher)\n",
    "    print('-----------')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rdflib.term.Variable('rel'), rdflib.term.Variable('dom'), rdflib.term.Variable('ran')]\n"
     ]
    }
   ],
   "source": [
    "rels_query = \"\"\"\n",
    "prefix owl: <http://www.w3.org/2002/07/owl#> \n",
    "prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> \n",
    "\n",
    "SELECT DISTINCT ?rel ?dom ?ran #(GROUP_CONCAT(DISTINCT ?relTriplet ; SEPARATOR=\",\") AS ?rels)\n",
    "WHERE {\n",
    "    ?rel a ?propertyClass .\n",
    "    filter(?propertyClass in (rdf:Property, owl:ObjectProperty, owl:FunctionalProperty, owl:AsymmetricProperty, \n",
    "           owl:InverseFunctionalProperty, owl:IrreflexiveProperty, owl:ReflexiveProperty, owl:SymmetricProperty, owl:TransitiveProperty))\n",
    "    \n",
    "    ?rel rdfs:domain ?dom ;\n",
    "      rdfs:range ?ran .\n",
    "    \n",
    "    #BIND (concat(str(?rel),';',str(?dom),';',str(?range)) AS ?relTriplet)\n",
    "    \n",
    "  }\"\"\"\n",
    "\n",
    "query_result_relations = g.query(rels_query)\n",
    "print(query_result_relations.vars)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cypher-item:\n",
      " unwind $records AS record\n",
      "cypher-item:\n",
      " match (source:Event { `<id_prop>`: record.`<col with source id>`} )\n",
      "cypher-item:\n",
      " match (target:Station { `<id_prop>`: record.`<col with target id>`} )\n",
      "cypher-item:\n",
      " merge (source)-[r:`affects`]->(target)\n",
      "cypher-item:\n",
      " return count(*) as total\n",
      "---------------------------------------\n",
      "cypher-item:\n",
      " unwind $records AS record\n",
      "cypher-item:\n",
      " match (source:Station { `<id_prop>`: record.`<col with source id>`} )\n",
      "cypher-item:\n",
      " match (target:Station { `<id_prop>`: record.`<col with target id>`} )\n",
      "cypher-item:\n",
      " merge (source)-[r:`link`]->(target)\n",
      "cypher-item:\n",
      " return count(*) as total\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for row in query_result_relations:\n",
    "    cypher = []\n",
    "    cypher.append(\"unwind $records AS record\")\n",
    "    cypher.append(\"match (source:\" + getLocalPart(row.dom) + \" { `<id_prop>`: record.`<col with source id>`} )\")\n",
    "    cypher.append(\"match (target:\" + getLocalPart(row.ran) + \" { `<id_prop>`: record.`<col with target id>`} )\")\n",
    "    cypher.append(\"merge (source)-[r:`\"+ getLocalPart(row.rel) +\"`]->(target)\")\n",
    "    cypher.append(\"return count(*) as total\") \n",
    "    for item in cypher:\n",
    "        print('cypher-item:\\n', item)\n",
    "    print('---------------------------------------')\n",
    "    cypher_list.append(' \\n'.join(cypher))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "unwind $records AS record \n",
      "merge (n:Event { `<id_prop>`: record.`<col with id>`} ) \n",
      "set n.eventDescription = record.`<col with value for eventDescription>` \n",
      "set n.eventId = record.`<col with value for eventId>` \n",
      "set n.eventType = record.`<col with value for eventType>` \n",
      "return count(*) as total\n",
      "\n",
      "\n",
      "unwind $records AS record \n",
      "merge (n:Station { `<id_prop>`: record.`<col with id>`} ) \n",
      "set n.lat = record.`<col with value for lat>` \n",
      "set n.long = record.`<col with value for long>` \n",
      "set n.stationAddress = record.`<col with value for stationAddress>` \n",
      "set n.stationCode = record.`<col with value for stationCode>` \n",
      "set n.stationName = record.`<col with value for stationName>` \n",
      "return count(*) as total\n",
      "\n",
      "\n",
      "unwind $records AS record \n",
      "match (source:Event { `<id_prop>`: record.`<col with source id>`} ) \n",
      "match (target:Station { `<id_prop>`: record.`<col with target id>`} ) \n",
      "merge (source)-[r:`affects`]->(target) \n",
      "return count(*) as total\n",
      "\n",
      "\n",
      "unwind $records AS record \n",
      "match (source:Station { `<id_prop>`: record.`<col with source id>`} ) \n",
      "match (target:Station { `<id_prop>`: record.`<col with target id>`} ) \n",
      "merge (source)-[r:`link`]->(target) \n",
      "return count(*) as total\n"
     ]
    }
   ],
   "source": [
    "for q in cypher_list:\n",
    "    print(\"\\n\\n\" + q)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "railMappings = {}\n",
    "\n",
    "stationMapping = {}\n",
    "stationMapping[\"@fileName\"] = \"https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-stations-all.csv\"\n",
    "stationMapping[\"@uniqueId\"] = \"stationCode\"\n",
    "stationMapping[\"lat\"] = \"lat\"\n",
    "stationMapping[\"long\"] = \"long\"\n",
    "stationMapping[\"stationAddress\"] = \"address\"\n",
    "stationMapping[\"stationCode\"] = \"crs\"\n",
    "stationMapping[\"stationName\"] = \"name\"\n",
    "railMappings[\"Station\"] = stationMapping\n",
    "\n",
    "eventMapping = {}\n",
    "eventMapping[\"@fileName\"] = \"https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-events.csv\"\n",
    "eventMapping[\"@uniqueId\"] = \"eventId\"\n",
    "eventMapping[\"eventDescription\"] = \"desc\"\n",
    "eventMapping[\"eventId\"] = \"id\"\n",
    "eventMapping[\"timestamp\"] = \"ts\"\n",
    "eventMapping[\"eventType\"] = \"type\"\n",
    "railMappings[\"Event\"] = eventMapping\n",
    "\n",
    "linkMapping = {}\n",
    "linkMapping[\"@fileName\"] = \"https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-station-links.csv\"\n",
    "linkMapping[\"@from\"] = \"origin\"\n",
    "linkMapping[\"@to\"] = \"destination\"\n",
    "railMappings[\"link\"] = linkMapping\n",
    "\n",
    "affectsMapping = {}\n",
    "affectsMapping[\"@fileName\"] = \"https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-events.csv\"\n",
    "affectsMapping[\"@from\"] = \"id\"\n",
    "affectsMapping[\"@to\"] = \"Station\"\n",
    "railMappings[\"affects\"] = affectsMapping\n",
    "\n",
    "# show it?\n",
    "railMappings\n",
    "from pprint import pprint\n",
    "pprint(railMappings)"
   ],
   "metadata": {
    "id": "F4GP70jAf8v2"
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Event': {'@fileName': 'https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-events.csv',\n",
      "           '@uniqueId': 'eventId',\n",
      "           'eventDescription': 'desc',\n",
      "           'eventId': 'id',\n",
      "           'eventType': 'type',\n",
      "           'timestamp': 'ts'},\n",
      " 'Station': {'@fileName': 'https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-stations-all.csv',\n",
      "             '@uniqueId': 'stationCode',\n",
      "             'lat': 'lat',\n",
      "             'long': 'long',\n",
      "             'stationAddress': 'address',\n",
      "             'stationCode': 'crs',\n",
      "             'stationName': 'name'},\n",
      " 'affects': {'@fileName': 'https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-events.csv',\n",
      "             '@from': 'id',\n",
      "             '@to': 'Station'},\n",
      " 'link': {'@fileName': 'https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-station-links.csv',\n",
      "          '@from': 'origin',\n",
      "          '@to': 'destination'}}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#copy of previous but using the mappings\n",
    "def getLoadersFromOnto(onto, rdf_format, mappings):\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(onto, format=rdf_format)\n",
    "\n",
    "    classes_and_props_query = \"\"\"\n",
    "    prefix owl: <http://www.w3.org/2002/07/owl#> \n",
    "    prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> \n",
    "    \n",
    "    SELECT DISTINCT ?curi (GROUP_CONCAT(DISTINCT ?propTypePair ; SEPARATOR=\",\") AS ?props)\n",
    "    WHERE {\n",
    "      ?curi rdf:type owl:Class .\n",
    "      optional { \n",
    "        ?prop rdfs:domain ?curi ;\n",
    "          a owl:DatatypeProperty ;\n",
    "          rdfs:range ?range .\n",
    "        BIND (concat(str(?prop),';',str(?range)) AS ?propTypePair)\n",
    "      }\n",
    "    } GROUP BY ?curi  \"\"\"\n",
    "    \n",
    "    cypher_import = {}\n",
    "    export_ns = set()\n",
    "    export_mappings = {}\n",
    "    \n",
    "    for row in g.query(classes_and_props_query):\n",
    "        export_ns.add(getNamespacePart(row.curi))\n",
    "        export_mappings[getLocalPart(row.curi)] = str(row.curi)\n",
    "        cypher = []\n",
    "        cypher.append(\"unwind $records AS record\")\n",
    "        cypher.append(\"merge (n:\" + getLocalPart(row.curi) + \" { `\" + mappings[getLocalPart(row.curi)][\"@uniqueId\"] + \"`: record.`\" + mappings[getLocalPart(row.curi)][mappings[getLocalPart(row.curi)][\"@uniqueId\"]] + \"`} )\")\n",
    "        print(\"classes_and_props_query:\", \"merge (n:\" + getLocalPart(row.curi) + \" { `\" + mappings[getLocalPart(row.curi)][\"@uniqueId\"] + \"`: record.`\" + mappings[getLocalPart(row.curi)][mappings[getLocalPart(row.curi)][\"@uniqueId\"]] + \"`} )\")\n",
    "        for pair in row.props.split(\",\"):      \n",
    "            propName = pair.split(\";\")[0]\n",
    "            propType = pair.split(\";\")[1]\n",
    "            export_ns.add(getNamespacePart(propName))\n",
    "            export_mappings[getLocalPart(propName)] = propName\n",
    "            #if a mapping (a column in the source file) is defined for the property and property is not a unique id\n",
    "            if getLocalPart(propName) in mappings[getLocalPart(row.curi)] and getLocalPart(propName) != mappings[getLocalPart(row.curi)][\"@uniqueId\"]:\n",
    "                cypher.append(\"set n.\" + getLocalPart(propName) + \" = record.`\" + mappings[getLocalPart(row.curi)][getLocalPart(propName)] + \"`\")\n",
    "        cypher.append(\"return count(*) as total\") \n",
    "        cypher_import[getLocalPart(row.curi)] = ' \\n'.join(cypher)\n",
    "        # print('CYPHER in classes_and_props_query:\\n', cypher)\n",
    "\n",
    "\n",
    "    rels_query = \"\"\"\n",
    "    prefix owl: <http://www.w3.org/2002/07/owl#> \n",
    "    prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> \n",
    "    \n",
    "    SELECT DISTINCT ?rel ?dom ?ran #(GROUP_CONCAT(DISTINCT ?relTriplet ; SEPARATOR=\",\") AS ?rels)\n",
    "    WHERE {\n",
    "      ?rel a ?propertyClass .\n",
    "      filter(?propertyClass in (rdf:Property, owl:ObjectProperty, owl:FunctionalProperty, owl:AsymmetricProperty, \n",
    "            owl:InverseFunctionalProperty, owl:IrreflexiveProperty, owl:ReflexiveProperty, owl:SymmetricProperty, owl:TransitiveProperty))\n",
    "      \n",
    "      ?rel rdfs:domain ?dom ;\n",
    "        rdfs:range ?ran .\n",
    "      \n",
    "      #BIND (concat(str(?rel),';',str(?dom),';',str(?range)) AS ?relTriplet)\n",
    "      \n",
    "    }\"\"\"\n",
    "\n",
    "    for row in g.query(rels_query):\n",
    "        export_ns.add(getNamespacePart(row.rel))\n",
    "        export_mappings[getLocalPart(row.rel)] = str(row.rel)\n",
    "        cypher = []\n",
    "        cypher.append(\"unwind $records AS record\")\n",
    "        cypher.append(\"match (source:\" + getLocalPart(row.dom) + \" { `\" + mappings[getLocalPart(row.dom)][\"@uniqueId\"] + \"`: record.`\" + mappings[getLocalPart(row.rel)][\"@from\"] + \"`} )\")\n",
    "        print(\"rels_query:\", \"match (source:\" + getLocalPart(row.dom) + \" { `\" + mappings[getLocalPart(row.dom)][\"@uniqueId\"] + \"`: record.`\" + mappings[getLocalPart(row.rel)][\"@from\"] + \"`} )\")\n",
    "        cypher.append(\"match (target:\" + getLocalPart(row.ran) + \" { `\" + mappings[getLocalPart(row.ran)][\"@uniqueId\"] + \"`: record.`\" + mappings[getLocalPart(row.rel)][\"@to\"] + \"`} )\")\n",
    "        cypher.append(\"merge (source)-[r:`\"+ getLocalPart(row.rel) +\"`]->(target)\")\n",
    "        cypher.append(\"return count(*) as total\") \n",
    "        cypher_import[getLocalPart(row.rel)] = ' \\n'.join(cypher)\n",
    "        # print('CYPHER in rels_query:\\n', cypher)\n",
    "\n",
    "\n",
    "    nscount = 0\n",
    "    mapping_export_cypher = []\n",
    "    \n",
    "    for ns in export_ns:\n",
    "        print('----- nsprefixes')\n",
    "        print(\"call n10s.nsprefixes.add('ns\" + str(nscount) + \"','\" + ns + \"');\")\n",
    "        mapping_export_cypher.append(\"call n10s.nsprefixes.add('ns\" + str(nscount) + \"','\" + ns + \"');\")\n",
    "        nscount+=1\n",
    "    \n",
    "    for k in export_mappings.keys():\n",
    "        mapping_export_cypher.append(\"call n10s.mapping.add('\" + export_mappings[k] + \"','\" + k + \"');\")\n",
    "    \n",
    "    return cypher_import ,  mapping_export_cypher\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes_and_props_query: merge (n:Event { `eventId`: record.`id`} )\n",
      "classes_and_props_query: merge (n:Station { `stationCode`: record.`crs`} )\n",
      "rels_query: match (source:Event { `eventId`: record.`id`} )\n",
      "rels_query: match (source:Station { `stationCode`: record.`origin`} )\n",
      "----- nsprefixes\n",
      "call n10s.nsprefixes.add('ns0','http://onto.neo4j.com/rail#');\n",
      "#LOADERS:\n",
      "\n",
      "\n",
      "Event: file: https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-events.csv unwind $records AS record \n",
      "merge (n:Event { `eventId`: record.`id`} ) \n",
      "set n.eventDescription = record.`desc` \n",
      "set n.eventType = record.`type` \n",
      "return count(*) as total\n",
      "\n",
      "\n",
      "Station: file: https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-stations-all.csv unwind $records AS record \n",
      "merge (n:Station { `stationCode`: record.`crs`} ) \n",
      "set n.lat = record.`lat` \n",
      "set n.long = record.`long` \n",
      "set n.stationAddress = record.`address` \n",
      "set n.stationName = record.`name` \n",
      "return count(*) as total\n",
      "\n",
      "\n",
      "affects: file: https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-events.csv unwind $records AS record \n",
      "match (source:Event { `eventId`: record.`id`} ) \n",
      "match (target:Station { `stationCode`: record.`Station`} ) \n",
      "merge (source)-[r:`affects`]->(target) \n",
      "return count(*) as total\n",
      "\n",
      "\n",
      "link: file: https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-station-links.csv unwind $records AS record \n",
      "match (source:Station { `stationCode`: record.`origin`} ) \n",
      "match (target:Station { `stationCode`: record.`destination`} ) \n",
      "merge (source)-[r:`link`]->(target) \n",
      "return count(*) as total\n",
      "\n",
      "\n",
      "---------------------------------------------------------------\n",
      "#EXPORT MAPPINGS (for RDF API):\n",
      "\n",
      "\n",
      "call n10s.nsprefixes.add('ns0','http://onto.neo4j.com/rail#');\n",
      "call n10s.mapping.add('http://onto.neo4j.com/rail#Event','Event');\n",
      "call n10s.mapping.add('http://onto.neo4j.com/rail#eventDescription','eventDescription');\n",
      "call n10s.mapping.add('http://onto.neo4j.com/rail#eventId','eventId');\n",
      "call n10s.mapping.add('http://onto.neo4j.com/rail#eventType','eventType');\n",
      "call n10s.mapping.add('http://onto.neo4j.com/rail#Station','Station');\n",
      "call n10s.mapping.add('http://onto.neo4j.com/rail#lat','lat');\n",
      "call n10s.mapping.add('http://onto.neo4j.com/rail#long','long');\n",
      "call n10s.mapping.add('http://onto.neo4j.com/rail#stationAddress','stationAddress');\n",
      "call n10s.mapping.add('http://onto.neo4j.com/rail#stationCode','stationCode');\n",
      "call n10s.mapping.add('http://onto.neo4j.com/rail#stationName','stationName');\n",
      "call n10s.mapping.add('http://onto.neo4j.com/rail#affects','affects');\n",
      "call n10s.mapping.add('http://onto.neo4j.com/rail#link','link');\n"
     ]
    }
   ],
   "source": [
    "cypher_import , mapping_defs = getLoadersFromOnto(\"https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/ontos/rail.ttl\",\"turtle\",railMappings)\n",
    "\n",
    "print(\"#LOADERS:\\n\\n\")\n",
    "for q in cypher_import.keys():\n",
    "  print(q + \": \\n\\nfile: \" + railMappings[q][\"@fileName\"] + \"\\n\\n\"+ cypher_import[q] + \"\\n\\n\")\n",
    "\n",
    "print('---------------------------------------------------------------')\n",
    "\n",
    "print(\"#EXPORT MAPPINGS (for RDF API):\\n\\n\")\n",
    "for md in mapping_defs:\n",
    "  print(md)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Utility function to write to Neo4j in batch mode.\n",
    "\n",
    "def insert_data(session, query, frame, batch_size = 500):\n",
    "    \n",
    "    print('QUERY:\\n', query)\n",
    "    print('---------------------------')\n",
    "    \n",
    "    total = 0\n",
    "    batch = 0\n",
    "    start = time.time()\n",
    "    result = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    while batch * batch_size < len(frame):\n",
    "        res = session.write_transaction( lambda tx: tx.run(query,\n",
    "                      parameters = {'records': frame[batch*batch_size:(batch+1)*batch_size].to_dict('records')}).data())\n",
    "\n",
    "        total += res[0]['total']\n",
    "        batch += 1\n",
    "        result = {\"total\":total, \n",
    "                  \"batches\":batch, \n",
    "                  \"time\":time.time()-start}\n",
    "        print(result)\n",
    "        \n",
    "    return result\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes_and_props_query: merge (n:Event { `eventId`: record.`id`} )\n",
      "classes_and_props_query: merge (n:Station { `stationCode`: record.`crs`} )\n",
      "rels_query: match (source:Event { `eventId`: record.`id`} )\n",
      "rels_query: match (source:Station { `stationCode`: record.`origin`} )\n",
      "----- nsprefixes\n",
      "call n10s.nsprefixes.add('ns0','http://onto.neo4j.com/rail#');\n",
      "CYPHER_IMPORT:\n",
      "{'Event': 'unwind $records AS record \\n'\n",
      "          'merge (n:Event { `eventId`: record.`id`} ) \\n'\n",
      "          'set n.eventDescription = record.`desc` \\n'\n",
      "          'set n.eventType = record.`type` \\n'\n",
      "          'return count(*) as total',\n",
      " 'Station': 'unwind $records AS record \\n'\n",
      "            'merge (n:Station { `stationCode`: record.`crs`} ) \\n'\n",
      "            'set n.lat = record.`lat` \\n'\n",
      "            'set n.long = record.`long` \\n'\n",
      "            'set n.stationAddress = record.`address` \\n'\n",
      "            'set n.stationName = record.`name` \\n'\n",
      "            'return count(*) as total',\n",
      " 'affects': 'unwind $records AS record \\n'\n",
      "            'match (source:Event { `eventId`: record.`id`} ) \\n'\n",
      "            'match (target:Station { `stationCode`: record.`Station`} ) \\n'\n",
      "            'merge (source)-[r:`affects`]->(target) \\n'\n",
      "            'return count(*) as total',\n",
      " 'link': 'unwind $records AS record \\n'\n",
      "         'match (source:Station { `stationCode`: record.`origin`} ) \\n'\n",
      "         'match (target:Station { `stationCode`: record.`destination`} ) \\n'\n",
      "         'merge (source)-[r:`link`]->(target) \\n'\n",
      "         'return count(*) as total'}\n",
      "----------------------------------\n",
      "[\"call n10s.nsprefixes.add('ns0','http://onto.neo4j.com/rail#');\",\n",
      " \"call n10s.mapping.add('http://onto.neo4j.com/rail#Event','Event');\",\n",
      " 'call '\n",
      " \"n10s.mapping.add('http://onto.neo4j.com/rail#eventDescription','eventDescription');\",\n",
      " \"call n10s.mapping.add('http://onto.neo4j.com/rail#eventId','eventId');\",\n",
      " \"call n10s.mapping.add('http://onto.neo4j.com/rail#eventType','eventType');\",\n",
      " \"call n10s.mapping.add('http://onto.neo4j.com/rail#Station','Station');\",\n",
      " \"call n10s.mapping.add('http://onto.neo4j.com/rail#lat','lat');\",\n",
      " \"call n10s.mapping.add('http://onto.neo4j.com/rail#long','long');\",\n",
      " 'call '\n",
      " \"n10s.mapping.add('http://onto.neo4j.com/rail#stationAddress','stationAddress');\",\n",
      " 'call '\n",
      " \"n10s.mapping.add('http://onto.neo4j.com/rail#stationCode','stationCode');\",\n",
      " 'call '\n",
      " \"n10s.mapping.add('http://onto.neo4j.com/rail#stationName','stationName');\",\n",
      " \"call n10s.mapping.add('http://onto.neo4j.com/rail#affects','affects');\",\n",
      " \"call n10s.mapping.add('http://onto.neo4j.com/rail#link','link');\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from neo4j import GraphDatabase, basic_auth\n",
    "\n",
    "uri = \"neo4j://localhost:7687\"\n",
    "auth = (\"neo4j\", os.getenv('NEO4J_PW'))\n",
    "driver = GraphDatabase.driver(uri, auth=auth)\n",
    "\n",
    "session = driver.session(database=\"neo4j\")\n",
    "\n",
    "cypher_import , mapping_defs = getLoadersFromOnto(\"https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/ontos/rail.ttl\",\"turtle\",railMappings)\n",
    "\n",
    "from pprint import pprint\n",
    "print('CYPHER_IMPORT:')\n",
    "pprint(cypher_import)\n",
    "print('----------------------------------')\n",
    "pprint(mapping_defs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to import Event from file https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-events.csv\n",
      "QUERY:\n",
      " unwind $records AS record \n",
      "merge (n:Event { `eventId`: record.`id`} ) \n",
      "set n.eventDescription = record.`desc` \n",
      "set n.eventType = record.`type` \n",
      "return count(*) as total\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 143, 'batches': 1, 'time': 4.165135622024536}\n",
      "{'total': 143, 'batches': 1, 'time': 4.165135622024536}\n",
      "about to import Station from file https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-stations-all.csv\n",
      "QUERY:\n",
      " unwind $records AS record \n",
      "merge (n:Station { `stationCode`: record.`crs`} ) \n",
      "set n.lat = record.`lat` \n",
      "set n.long = record.`long` \n",
      "set n.stationAddress = record.`address` \n",
      "set n.stationName = record.`name` \n",
      "return count(*) as total\n",
      "---------------------------\n",
      "{'total': 300, 'batches': 1, 'time': 0.1125643253326416}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n",
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 600, 'batches': 2, 'time': 0.3295478820800781}\n",
      "{'total': 900, 'batches': 3, 'time': 0.5256023406982422}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n",
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 1200, 'batches': 4, 'time': 0.8745472431182861}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 1500, 'batches': 5, 'time': 1.3125479221343994}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 1800, 'batches': 6, 'time': 1.747546672821045}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 2100, 'batches': 7, 'time': 2.1865475177764893}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 2400, 'batches': 8, 'time': 2.7285468578338623}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 2593, 'batches': 9, 'time': 3.1896042823791504}\n",
      "{'total': 2593, 'batches': 9, 'time': 3.1896042823791504}\n",
      "about to import affects from file https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-events.csv\n",
      "QUERY:\n",
      " unwind $records AS record \n",
      "match (source:Event { `eventId`: record.`id`} ) \n",
      "match (target:Station { `stationCode`: record.`Station`} ) \n",
      "merge (source)-[r:`affects`]->(target) \n",
      "return count(*) as total\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 143, 'batches': 1, 'time': 0.36203956604003906}\n",
      "{'total': 143, 'batches': 1, 'time': 0.36203956604003906}\n",
      "about to import link from file https://raw.githubusercontent.com/jbarrasa/goingmeta/main/session5/data/nr-station-links.csv\n",
      "QUERY:\n",
      " unwind $records AS record \n",
      "match (source:Station { `stationCode`: record.`origin`} ) \n",
      "match (target:Station { `stationCode`: record.`destination`} ) \n",
      "merge (source)-[r:`link`]->(target) \n",
      "return count(*) as total\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 300, 'batches': 1, 'time': 1.1340293884277344}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 600, 'batches': 2, 'time': 2.0850327014923096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 898, 'batches': 3, 'time': 3.0190353393554688}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 1198, 'batches': 4, 'time': 4.110044240951538}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 1498, 'batches': 5, 'time': 5.424029350280762}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 1796, 'batches': 6, 'time': 6.352037668228149}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 2095, 'batches': 7, 'time': 7.702035188674927}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 2394, 'batches': 8, 'time': 8.66907787322998}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 2694, 'batches': 9, 'time': 10.049068450927734}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 2994, 'batches': 10, 'time': 11.02712345123291}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 3292, 'batches': 11, 'time': 12.05907130241394}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 3592, 'batches': 12, 'time': 13.035079002380371}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 3892, 'batches': 13, 'time': 13.993124008178711}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 4191, 'batches': 14, 'time': 14.932070255279541}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 4490, 'batches': 15, 'time': 15.86007308959961}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 4785, 'batches': 16, 'time': 16.80207872390747}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 5082, 'batches': 17, 'time': 17.76506996154785}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 5381, 'batches': 18, 'time': 18.809684991836548}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 5681, 'batches': 19, 'time': 19.772650957107544}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portfolio-Resolution\\AppData\\Local\\Temp\\ipykernel_11812\\2121215017.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  res = session.write_transaction( lambda tx: tx.run(query,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 5782, 'batches': 20, 'time': 20.115660667419434}\n",
      "{'total': 5782, 'batches': 20, 'time': 20.115660667419434}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for q in cypher_import.keys():\n",
    "    print(\"about to import \" + q + \" from file \" + railMappings[q][\"@fileName\"])\n",
    "    df = pd.read_csv(railMappings[q][\"@fileName\"])\n",
    "    result = insert_data(session, cypher_import[q], df, batch_size = 300) \n",
    "    print(result)\n",
    "\n",
    "for md in mapping_defs:\n",
    "    session.run(md)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
